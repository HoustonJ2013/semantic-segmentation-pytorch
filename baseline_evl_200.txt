Namespace(arch_decoder='c1_bilinear', arch_encoder='resnet34_dilated8', batch_size=1, ckpt='./ckpt', fc_dim=512, id='baseline-resnet34_dilated8-c1_bilinear-ngpus1-batchSize8-imgSize384-segSize384-lr_encoder0.001-lr_decoder0.01-epoch5-decay0.0001', imgSize=-1, list_val='./data/Jingbo_val.txt', num_class=150, num_val=-1, result='./result', root_img='./data/ADEChallengeData2016/images', root_seg='./data/ADEChallengeData2016/annotations', segSize=-1, suffix='_best.pth', visualize=0)
Loading weights for net_encoder
Loading weights for net_decoder
# samples: 200
[2017-12-28 02:25:11] iter 0, loss: 0.875694990158, accuracy: 0.685419666875
[2017-12-28 02:25:12] iter 1, loss: 1.02031445503, accuracy: 0.64747685487
[2017-12-28 02:25:12] iter 2, loss: 1.07822906971, accuracy: 0.631142570753
[2017-12-28 02:25:15] iter 3, loss: 0.324446290731, accuracy: 0.92225220419
[2017-12-28 02:25:17] iter 4, loss: 1.05157828331, accuracy: 0.628663601197
[2017-12-28 02:25:17] iter 5, loss: 0.671210467815, accuracy: 0.792993032912
[2017-12-28 02:25:18] iter 6, loss: 0.278372913599, accuracy: 0.920877616222
[2017-12-28 02:25:20] iter 7, loss: 0.585966706276, accuracy: 0.79551434951
[2017-12-28 02:25:20] iter 8, loss: 0.372645139694, accuracy: 0.896523151909
[2017-12-28 02:25:22] iter 9, loss: 0.320529967546, accuracy: 0.883210981001
[2017-12-28 02:25:24] iter 10, loss: 0.693348824978, accuracy: 0.731924004457
[2017-12-28 02:25:27] iter 11, loss: 0.424913406372, accuracy: 0.912664238076
[2017-12-28 02:25:29] iter 12, loss: 0.746285617352, accuracy: 0.775456829111
[2017-12-28 02:25:31] iter 13, loss: 0.495283126831, accuracy: 0.926713608277
[2017-12-28 02:25:32] iter 14, loss: 1.75622665882, accuracy: 0.608604346988
[2017-12-28 02:25:32] iter 15, loss: 1.11604392529, accuracy: 0.702537894962
[2017-12-28 02:25:33] iter 16, loss: 0.523807942867, accuracy: 0.851166645815
[2017-12-28 02:25:35] iter 17, loss: 0.783258676529, accuracy: 0.790048941059
[2017-12-28 02:25:35] iter 18, loss: 0.400195807219, accuracy: 0.892422052447
[2017-12-28 02:25:37] iter 19, loss: 0.900089681149, accuracy: 0.747678379215
[2017-12-28 02:25:38] iter 20, loss: 1.23609876633, accuracy: 0.558831242323
[2017-12-28 02:25:38] iter 21, loss: 1.19664132595, accuracy: 0.585397273299
[2017-12-28 02:25:40] iter 22, loss: 1.59758210182, accuracy: 0.534488926881
[2017-12-28 02:25:40] iter 23, loss: 0.819219470024, accuracy: 0.630559286188
[2017-12-28 02:25:42] iter 24, loss: 0.702407538891, accuracy: 0.695017939105
[2017-12-28 02:25:45] iter 25, loss: 1.02773940563, accuracy: 0.700846624941
[2017-12-28 02:25:45] iter 26, loss: 1.67703616619, accuracy: 0.531555610297
[2017-12-28 02:25:47] iter 27, loss: 2.37753725052, accuracy: 0.416592102424
[2017-12-28 02:25:49] iter 28, loss: 0.363205492496, accuracy: 0.866025312301
[2017-12-28 02:25:50] iter 29, loss: 2.55417656898, accuracy: 0.426301320877
[2017-12-28 02:25:50] iter 30, loss: 1.63383674622, accuracy: 0.368320421136
[2017-12-28 02:25:50] iter 31, loss: 1.08480226994, accuracy: 0.60407804878
[2017-12-28 02:25:51] iter 32, loss: 0.283319950104, accuracy: 0.895827465497
[2017-12-28 02:25:52] iter 33, loss: 1.47153604031, accuracy: 0.581422133997
[2017-12-28 02:25:54] iter 34, loss: 0.242820799351, accuracy: 0.947014837342
[2017-12-28 02:25:57] iter 35, loss: 0.507460057735, accuracy: 0.86895583592
[2017-12-28 02:25:57] iter 36, loss: 0.721913933754, accuracy: 0.786633890772
[2017-12-28 02:25:59] iter 37, loss: 0.338657230139, accuracy: 0.926473798015
[2017-12-28 02:26:01] iter 38, loss: 0.450504422188, accuracy: 0.85190487849
[2017-12-28 02:26:03] iter 39, loss: 0.542850732803, accuracy: 0.829238757444
[2017-12-28 02:26:04] iter 40, loss: 0.713222026825, accuracy: 0.754518705338
[2017-12-28 02:26:06] iter 41, loss: 0.567677617073, accuracy: 0.825024471799
[2017-12-28 02:26:06] iter 42, loss: 1.08121466637, accuracy: 0.565649175575
[2017-12-28 02:26:07] iter 43, loss: 0.314908742905, accuracy: 0.910658799159
[2017-12-28 02:26:07] iter 44, loss: 0.381201952696, accuracy: 0.847783689326
[2017-12-28 02:26:09] iter 45, loss: 0.866672217846, accuracy: 0.786708167165
[2017-12-28 02:26:09] iter 46, loss: 1.0183801651, accuracy: 0.734103927144
[2017-12-28 02:26:10] iter 47, loss: 1.18215703964, accuracy: 0.578996797635
[2017-12-28 02:26:11] iter 48, loss: 1.47992610931, accuracy: 0.590781457618
[2017-12-28 02:26:13] iter 49, loss: 0.90250647068, accuracy: 0.814364000891
[2017-12-28 02:26:15] iter 50, loss: 0.942932486534, accuracy: 0.676420477427
[2017-12-28 02:26:16] iter 51, loss: 2.10366725922, accuracy: 0.361209046578
[2017-12-28 02:26:19] iter 52, loss: 0.534868299961, accuracy: 0.841295958893
[2017-12-28 02:26:20] iter 53, loss: 1.35183298588, accuracy: 0.649996261443
[2017-12-28 02:26:21] iter 54, loss: 0.322075217962, accuracy: 0.931543666603
[2017-12-28 02:26:23] iter 55, loss: 0.430587828159, accuracy: 0.878252116457
[2017-12-28 02:26:25] iter 56, loss: 0.294170796871, accuracy: 0.949401613829
[2017-12-28 02:26:27] iter 57, loss: 0.792297422886, accuracy: 0.742520930525
[2017-12-28 02:26:28] iter 58, loss: 0.101516574621, accuracy: 0.984367539928
[2017-12-28 02:26:30] iter 59, loss: 0.900375366211, accuracy: 0.745716464608
[2017-12-28 02:26:31] iter 60, loss: 1.28605175018, accuracy: 0.658877007152
[2017-12-28 02:26:33] iter 61, loss: 1.05785965919, accuracy: 0.663344317057
[2017-12-28 02:26:34] iter 62, loss: 1.23984980583, accuracy: 0.604934210526
[2017-12-28 02:26:36] iter 63, loss: 0.446483641863, accuracy: 0.859323078375
[2017-12-28 02:26:37] iter 64, loss: 0.655413627625, accuracy: 0.852451213708
[2017-12-28 02:26:39] iter 65, loss: 1.74963569641, accuracy: 0.399496982927
[2017-12-28 02:26:41] iter 66, loss: 0.557537138462, accuracy: 0.844275173701
[2017-12-28 02:26:43] iter 67, loss: 1.04885911942, accuracy: 0.697216437375
[2017-12-28 02:26:45] iter 68, loss: 0.33398938179, accuracy: 0.889786560717
[2017-12-28 02:26:47] iter 69, loss: 0.24599955976, accuracy: 0.945078449912
[2017-12-28 02:26:49] iter 70, loss: 0.843558967113, accuracy: 0.771627365357
[2017-12-28 02:26:49] iter 71, loss: 0.30931609869, accuracy: 0.950891562772
[2017-12-28 02:26:51] iter 72, loss: 0.708787739277, accuracy: 0.804091671851
[2017-12-28 02:26:52] iter 73, loss: 1.53094315529, accuracy: 0.575881282856
[2017-12-28 02:26:52] iter 74, loss: 0.441356420517, accuracy: 0.875308641975
[2017-12-28 02:26:54] iter 75, loss: 2.00376296043, accuracy: 0.344894133216
[2017-12-28 02:26:56] iter 76, loss: 1.28943514824, accuracy: 0.692307692308
[2017-12-28 02:26:57] iter 77, loss: 0.932429790497, accuracy: 0.676823736592
[2017-12-28 02:26:57] iter 78, loss: 0.843356013298, accuracy: 0.708358373999
[2017-12-28 02:26:58] iter 79, loss: 1.04507851601, accuracy: 0.767838066411
[2017-12-28 02:26:59] iter 80, loss: 0.776909291744, accuracy: 0.808259342448
[2017-12-28 02:27:01] iter 81, loss: 0.901212573051, accuracy: 0.738304362959
[2017-12-28 02:27:02] iter 82, loss: 1.45449578762, accuracy: 0.516741207563
[2017-12-28 02:27:03] iter 83, loss: 1.57197964191, accuracy: 0.590046060265
[2017-12-28 02:27:03] iter 84, loss: 0.59044277668, accuracy: 0.757602228339
[2017-12-28 02:27:04] iter 85, loss: 1.1663980484, accuracy: 0.545135292495
[2017-12-28 02:27:07] iter 86, loss: 0.45765915513, accuracy: 0.889228703021
[2017-12-28 02:27:09] iter 87, loss: 0.35438606143, accuracy: 0.880034575668
[2017-12-28 02:27:09] iter 88, loss: 0.579425871372, accuracy: 0.861230329041
[2017-12-28 02:27:11] iter 89, loss: 1.23110568523, accuracy: 0.617796552387
[2017-12-28 02:27:12] iter 90, loss: 1.78512263298, accuracy: 0.520220278345
[2017-12-28 02:27:14] iter 91, loss: 0.580251634121, accuracy: 0.849337963404
[2017-12-28 02:27:15] iter 92, loss: 0.616419732571, accuracy: 0.883851224469
[2017-12-28 02:27:15] iter 93, loss: 0.647408366203, accuracy: 0.75096630736
[2017-12-28 02:27:17] iter 94, loss: 1.09045815468, accuracy: 0.755172299919
[2017-12-28 02:27:19] iter 95, loss: 2.14242458344, accuracy: 0.450156987242
[2017-12-28 02:27:20] iter 96, loss: 1.52506041527, accuracy: 0.666406229055
[2017-12-28 02:27:22] iter 97, loss: 0.570879936218, accuracy: 0.903839323768
[2017-12-28 02:27:24] iter 98, loss: 1.47774744034, accuracy: 0.712877699706
[2017-12-28 02:27:25] iter 99, loss: 3.4280796051, accuracy: 0.206987189779
[2017-12-28 02:27:26] iter 100, loss: 1.85766518116, accuracy: 0.417844349093
[2017-12-28 02:27:26] iter 101, loss: 2.53851485252, accuracy: 0.0867606470537
[2017-12-28 02:27:28] iter 102, loss: 1.15558636189, accuracy: 0.719504057741
[2017-12-28 02:27:28] iter 103, loss: 1.07169401646, accuracy: 0.686786805512
[2017-12-28 02:27:30] iter 104, loss: 0.448390781879, accuracy: 0.873168856894
[2017-12-28 02:27:31] iter 105, loss: 0.64084148407, accuracy: 0.801251422071
[2017-12-28 02:27:34] iter 106, loss: 2.18342614174, accuracy: 0.548498570332
[2017-12-28 02:27:34] iter 107, loss: 0.270467668772, accuracy: 0.956021197539
[2017-12-28 02:27:36] iter 108, loss: 0.704025745392, accuracy: 0.849268050616
[2017-12-28 02:27:38] iter 109, loss: 1.10499083996, accuracy: 0.659992400105
[2017-12-28 02:27:40] iter 110, loss: 0.126749366522, accuracy: 0.983343559208
[2017-12-28 02:27:42] iter 111, loss: 0.160151913762, accuracy: 0.963995559225
[2017-12-28 02:27:44] iter 112, loss: 1.45240068436, accuracy: 0.612670593724
[2017-12-28 02:27:44] iter 113, loss: 0.813613951206, accuracy: 0.760854466036
[2017-12-28 02:27:45] iter 114, loss: 1.27633452415, accuracy: 0.667268332196
[2017-12-28 02:27:47] iter 115, loss: 0.742726922035, accuracy: 0.754204744171
[2017-12-28 02:27:48] iter 116, loss: 0.146659091115, accuracy: 0.973200942273
[2017-12-28 02:27:50] iter 117, loss: 1.0318915844, accuracy: 0.698611916953
[2017-12-28 02:27:51] iter 118, loss: 0.873882472515, accuracy: 0.718299978124
[2017-12-28 02:27:53] iter 119, loss: 0.607157349586, accuracy: 0.795785382212
[2017-12-28 02:27:55] iter 120, loss: 1.37620568275, accuracy: 0.58674202622
[2017-12-28 02:27:56] iter 121, loss: 0.957284271717, accuracy: 0.74589400402
[2017-12-28 02:27:57] iter 122, loss: 1.16558659077, accuracy: 0.602648258634
[2017-12-28 02:27:57] iter 123, loss: 0.32143458724, accuracy: 0.949941079425
[2017-12-28 02:27:59] iter 124, loss: 0.490806102753, accuracy: 0.867902692252
[2017-12-28 02:28:01] iter 125, loss: 1.37689459324, accuracy: 0.630593275874
[2017-12-28 02:28:02] iter 126, loss: 0.304074645042, accuracy: 0.912429245837
[2017-12-28 02:28:03] iter 127, loss: 0.503766059875, accuracy: 0.899587717887
[2017-12-28 02:28:04] iter 128, loss: 0.130478471518, accuracy: 0.968573859039
[2017-12-28 02:28:06] iter 129, loss: 0.597087085247, accuracy: 0.754146318585
[2017-12-28 02:28:07] iter 130, loss: 0.316086083651, accuracy: 0.92533226553
[2017-12-28 02:28:08] iter 131, loss: 0.497207671404, accuracy: 0.87791874503
[2017-12-28 02:28:08] iter 132, loss: 0.741751849651, accuracy: 0.759382994733
[2017-12-28 02:28:10] iter 133, loss: 0.243516415358, accuracy: 0.955546082582
[2017-12-28 02:28:12] iter 134, loss: 0.280280590057, accuracy: 0.929736825292
[2017-12-28 02:28:15] iter 135, loss: 0.681699335575, accuracy: 0.832395702341
[2017-12-28 02:28:16] iter 136, loss: 0.32537099719, accuracy: 0.893884155827
[2017-12-28 02:28:18] iter 137, loss: 1.47869277, accuracy: 0.581645760161
[2017-12-28 02:28:20] iter 138, loss: 1.26968741417, accuracy: 0.564076827368
[2017-12-28 02:28:20] iter 139, loss: 0.25054192543, accuracy: 0.926432996158
[2017-12-28 02:28:22] iter 140, loss: 0.988327860832, accuracy: 0.609712770505
[2017-12-28 02:28:23] iter 141, loss: 1.27049911022, accuracy: 0.595065384487
[2017-12-28 02:28:25] iter 142, loss: 0.911542475224, accuracy: 0.648886491925
[2017-12-28 02:28:25] iter 143, loss: 0.853190898895, accuracy: 0.640412260148
[2017-12-28 02:28:27] iter 144, loss: 0.498923182487, accuracy: 0.835055439122
[2017-12-28 02:28:29] iter 145, loss: 0.163746580482, accuracy: 0.948646837766
[2017-12-28 02:28:30] iter 146, loss: 1.2827231884, accuracy: 0.655497366141
[2017-12-28 02:28:31] iter 147, loss: 0.450390577316, accuracy: 0.859238141037
[2017-12-28 02:28:31] iter 148, loss: 1.87615251541, accuracy: 0.311189371925
[2017-12-28 02:28:32] iter 149, loss: 0.470110267401, accuracy: 0.840628489887
[2017-12-28 02:28:33] iter 150, loss: 1.03903985023, accuracy: 0.682641756357
[2017-12-28 02:28:34] iter 151, loss: 0.856417298317, accuracy: 0.711002640977
[2017-12-28 02:28:34] iter 152, loss: 1.24051606655, accuracy: 0.56303599507
[2017-12-28 02:28:36] iter 153, loss: 0.285545229912, accuracy: 0.942591806012
[2017-12-28 02:28:37] iter 154, loss: 0.4269118011, accuracy: 0.763034058612
[2017-12-28 02:28:39] iter 155, loss: 0.883523583412, accuracy: 0.750670652899
[2017-12-28 02:28:41] iter 156, loss: 0.837123692036, accuracy: 0.833458007875
[2017-12-28 02:28:42] iter 157, loss: 0.44740486145, accuracy: 0.826493078003
[2017-12-28 02:28:44] iter 158, loss: 0.318894118071, accuracy: 0.898034522333
[2017-12-28 02:28:46] iter 159, loss: 0.92674189806, accuracy: 0.746616276501
[2017-12-28 02:28:48] iter 160, loss: 0.684453964233, accuracy: 0.878214850243
[2017-12-28 02:28:48] iter 161, loss: 1.60697114468, accuracy: 0.478322050063
[2017-12-28 02:28:49] iter 162, loss: 1.1797606945, accuracy: 0.716773487572
[2017-12-28 02:28:50] iter 163, loss: 0.477930247784, accuracy: 0.84922781863
[2017-12-28 02:28:51] iter 164, loss: 0.825364112854, accuracy: 0.861419850143
[2017-12-28 02:28:51] iter 165, loss: 1.33811604977, accuracy: 0.664257825878
[2017-12-28 02:28:52] iter 166, loss: 0.717712759972, accuracy: 0.891524529049
[2017-12-28 02:28:54] iter 167, loss: 0.424741894007, accuracy: 0.922060502118
[2017-12-28 02:28:56] iter 168, loss: 0.204417303205, accuracy: 0.937795815875
[2017-12-28 02:28:58] iter 169, loss: 0.990958929062, accuracy: 0.689152015558
[2017-12-28 02:28:59] iter 170, loss: 0.251511543989, accuracy: 1.0
[2017-12-28 02:28:59] iter 171, loss: 1.31007158756, accuracy: 0.570779361301
[2017-12-28 02:29:01] iter 172, loss: 1.85824167728, accuracy: 0.577790560464
[2017-12-28 02:29:02] iter 173, loss: 1.3980666399, accuracy: 0.617051305912
[2017-12-28 02:29:02] iter 174, loss: 1.83984661102, accuracy: 0.447489749431
[2017-12-28 02:29:03] iter 175, loss: 2.32390069962, accuracy: 0.37673591091
[2017-12-28 02:29:05] iter 176, loss: 1.14269828796, accuracy: 0.61732581402
[2017-12-28 02:29:06] iter 177, loss: 1.13198041916, accuracy: 0.543368933858
[2017-12-28 02:29:07] iter 178, loss: 1.42845547199, accuracy: 0.496404929219
[2017-12-28 02:29:08] iter 179, loss: 1.19720411301, accuracy: 0.636961736238
[2017-12-28 02:29:10] iter 180, loss: 0.337803721428, accuracy: 0.904435344177
[2017-12-28 02:29:11] iter 181, loss: 0.197339490056, accuracy: 0.969990243288
[2017-12-28 02:29:12] iter 182, loss: 1.80767381191, accuracy: 0.4556376743
[2017-12-28 02:29:14] iter 183, loss: 0.375196039677, accuracy: 0.911634230546
[2017-12-28 02:29:16] iter 184, loss: 0.459959954023, accuracy: 0.878931425439
[2017-12-28 02:29:17] iter 185, loss: 0.318813294172, accuracy: 0.957314052146
[2017-12-28 02:29:19] iter 186, loss: 0.65553176403, accuracy: 0.739479448593
[2017-12-28 02:29:21] iter 187, loss: 0.387484669685, accuracy: 0.88066928561
[2017-12-28 02:29:23] iter 188, loss: 0.636765539646, accuracy: 0.837860362803
[2017-12-28 02:29:23] iter 189, loss: 1.68351233006, accuracy: 0.259795150244
[2017-12-28 02:29:25] iter 190, loss: 0.599549710751, accuracy: 0.831760974172
[2017-12-28 02:29:27] iter 191, loss: 0.19859880209, accuracy: 0.953211485826
[2017-12-28 02:29:29] iter 192, loss: 1.20491218567, accuracy: 0.668677226805
[2017-12-28 02:29:32] iter 193, loss: 0.598180472851, accuracy: 0.764310086584
[2017-12-28 02:29:32] iter 194, loss: 0.136713579297, accuracy: 0.948985453247
[2017-12-28 02:29:32] iter 195, loss: 0.60875171423, accuracy: 0.835751497786
[2017-12-28 02:29:34] iter 196, loss: 1.70165503025, accuracy: 0.516146735895
[2017-12-28 02:29:36] iter 197, loss: 4.89511156082, accuracy: 0.19600557497
[2017-12-28 02:29:37] iter 198, loss: 1.45433485508, accuracy: 0.436317964509
[2017-12-28 02:29:37] iter 199, loss: 0.144526541233, accuracy: 0.949301802842
class [0], IoU: 0.655529499054
class [1], IoU: 0.747097432613
class [2], IoU: 0.909784436226
class [3], IoU: 0.646574676037
class [4], IoU: 0.76461648941
class [5], IoU: 0.730557143688
class [6], IoU: 0.816627860069
class [7], IoU: 0.706753373146
class [8], IoU: 0.557866632938
class [9], IoU: 0.675753295422
class [10], IoU: 0.396714478731
class [11], IoU: 0.508858501911
class [12], IoU: 0.705638170242
class [13], IoU: 0.392478585243
class [14], IoU: 0.224058151245
class [15], IoU: 0.402685582638
class [16], IoU: 0.662697315216
class [17], IoU: 0.364214301109
class [18], IoU: 0.613912880421
class [19], IoU: 0.34192147851
class [20], IoU: 0.797640383244
class [21], IoU: 0.647793292999
class [22], IoU: 0.665164351463
class [23], IoU: 0.197615772486
class [24], IoU: 0.47699996829
class [25], IoU: 0.0522966422141
class [26], IoU: 0.723089993
class [27], IoU: 0.366576552391
class [28], IoU: 0.210213929415
class [29], IoU: 0.0
class [30], IoU: 0.0455465428531
class [31], IoU: 0.125102117658
class [32], IoU: 0.327896863222
class [33], IoU: 0.198963537812
class [34], IoU: 0.257998406887
class [35], IoU: 0.194251656532
class [36], IoU: 0.284402817488
class [37], IoU: 0.444431006908
class [38], IoU: 0.0218336768448
class [39], IoU: 0.236936077476
class [40], IoU: 0.168980583549
class [41], IoU: 0.296383202076
class [42], IoU: 0.107188388705
class [43], IoU: 0.163136541843
class [44], IoU: 0.0107485828921
class [45], IoU: 0.501116931438
class [46], IoU: 0.0517213307321
class [47], IoU: 0.502304017544
class [48], IoU: 0.486201524734
class [49], IoU: 0.781239807606
class [50], IoU: 0.107780449092
class [51], IoU: 0.178257852793
class [52], IoU: 0.113145135343
class [53], IoU: 0.0636152923107
class [54], IoU: 0.0
class [55], IoU: 0.0
class [56], IoU: 0.816872656345
class [57], IoU: 0.347814381123
class [58], IoU: 0.286967933178
class [59], IoU: 0.48905736208
class [60], IoU: 0.465963572264
class [61], IoU: 0.491192549467
class [62], IoU: 0.217823252082
class [63], IoU: 0.203354924917
class [64], IoU: 0.164895161986
class [65], IoU: 0.585366666317
class [66], IoU: 0.19035667181
class [67], IoU: 0.335269927979
class [68], IoU: 0.0
class [69], IoU: 0.00336119323038
class [70], IoU: 0.482036501169
class [71], IoU: 0.0118551328778
class [72], IoU: 0.3879583776
class [73], IoU: 0.572224497795
class [74], IoU: 0.269955515862
class [75], IoU: 0.13926024735
class [76], IoU: 0.303967893124
class [77], IoU: 0.135735437274
class [78], IoU: 0.0
class [79], IoU: 0.0
class [80], IoU: 0.0
class [81], IoU: 0.0829453393817
class [82], IoU: 0.168360635638
class [83], IoU: 0.419413745403
class [84], IoU: 0.0413530915976
class [85], IoU: 0.33809247613
class [86], IoU: 0.149053111672
class [87], IoU: 0.0370113216341
class [88], IoU: 0.0
class [89], IoU: 0.222171604633
class [90], IoU: 0.0
class [91], IoU: 0.0
class [92], IoU: 0.0
class [93], IoU: 0.0391175970435
class [94], IoU: 0.0
class [95], IoU: 0.0
class [96], IoU: 0.0704013183713
class [97], IoU: 0.0
class [98], IoU: 0.356763809919
class [99], IoU: 0.0444893166423
class [100], IoU: 0.00148837210145
class [101], IoU: 0.0
class [102], IoU: 0.0
class [103], IoU: 0.0
class [104], IoU: 0.0
class [105], IoU: 0.00263698329218
class [106], IoU: 0.0
class [107], IoU: 0.720065832138
class [108], IoU: 0.0
class [109], IoU: 0.0
class [110], IoU: 0.0614075027406
class [111], IoU: 0.0
class [112], IoU: 0.0
class [113], IoU: 0.0
class [114], IoU: 0.0
class [115], IoU: 0.0
class [116], IoU: 0.581215441227
class [117], IoU: 0.430453330278
class [118], IoU: 0.0
class [119], IoU: 0.0
class [120], IoU: 0.0
class [121], IoU: 0.0415467172861
class [122], IoU: 5.3356616263e-05
class [123], IoU: 0.163685530424
class [124], IoU: 0.807878911495
class [125], IoU: 0.152329191566
class [126], IoU: 0.574362397194
class [127], IoU: 0.491281956434
class [128], IoU: 0.0
class [129], IoU: 0.0
class [130], IoU: 0.0300987046212
class [131], IoU: 0.0
class [132], IoU: 0.0
class [133], IoU: 0.0
class [134], IoU: 0.048504345119
class [135], IoU: 0.120158419013
class [136], IoU: 0.0316884368658
class [137], IoU: 0.0
class [138], IoU: 0.0925527065992
class [139], IoU: 0.278567403555
class [140], IoU: 0.0762476995587
class [141], IoU: 0.0
class [142], IoU: 0.0149295441806
class [143], IoU: 0.0
class [144], IoU: 0.147066816688
class [145], IoU: 0.0
class [146], IoU: 0.0
class [147], IoU: 0.00270334957168
class [148], IoU: 0.0
class [149], IoU: 0.0250283982605
[Eval Summary]:
Loss: 0.911628597006, Mean IoU: 0.236, Accurarcy: 74.50%
Evaluation Done!
