Namespace(arch_decoder='c1_bilinear', arch_encoder='resnet34_dilated8', batch_size=1, ckpt='./ckpt', fc_dim=512, id='Grayscale1-resnet34_dilated8-c1_bilinear-ngpus1-batchSize8-imgSize384-segSize384-lr_encoder0.001-lr_decoder0.01-epoch5-decay0.0001', imgSize=-1, list_val='./data/Jingbo_gray_val.txt', num_class=150, num_val=-1, result='./result', root_img='./data/ADEChallengeData2016/images', root_seg='./data/ADEChallengeData2016/annotations', segSize=-1, suffix='_best.pth', visualize=0)
Loading weights for net_encoder
Loading weights for net_decoder
# samples: 200
[2017-12-28 02:17:45] iter 0, loss: 0.804637074471, accuracy: 0.772464201288
[2017-12-28 02:17:45] iter 1, loss: 1.00443959236, accuracy: 0.680939496675
[2017-12-28 02:17:46] iter 2, loss: 1.00664043427, accuracy: 0.632175871499
[2017-12-28 02:17:48] iter 3, loss: 0.608639121056, accuracy: 0.824623528714
[2017-12-28 02:17:50] iter 4, loss: 0.861931145191, accuracy: 0.672067732282
[2017-12-28 02:17:51] iter 5, loss: 0.741708636284, accuracy: 0.802317671069
[2017-12-28 02:17:51] iter 6, loss: 0.28909021616, accuracy: 0.936509747669
[2017-12-28 02:17:53] iter 7, loss: 0.590472042561, accuracy: 0.802573359198
[2017-12-28 02:17:54] iter 8, loss: 0.384199887514, accuracy: 0.90448415922
[2017-12-28 02:17:56] iter 9, loss: 0.36974927783, accuracy: 0.879076882072
[2017-12-28 02:17:58] iter 10, loss: 0.717339158058, accuracy: 0.806345761686
[2017-12-28 02:18:01] iter 11, loss: 0.467392146587, accuracy: 0.870915567495
[2017-12-28 02:18:03] iter 12, loss: 0.732154846191, accuracy: 0.8027072418
[2017-12-28 02:18:05] iter 13, loss: 0.609568893909, accuracy: 0.925799524197
[2017-12-28 02:18:06] iter 14, loss: 1.69122028351, accuracy: 0.622079904952
[2017-12-28 02:18:06] iter 15, loss: 1.1621389389, accuracy: 0.702497865073
[2017-12-28 02:18:07] iter 16, loss: 0.587624490261, accuracy: 0.8947516577
[2017-12-28 02:18:09] iter 17, loss: 0.845955550671, accuracy: 0.793557977335
[2017-12-28 02:18:09] iter 18, loss: 0.463722556829, accuracy: 0.909750790197
[2017-12-28 02:18:11] iter 19, loss: 0.942218601704, accuracy: 0.771258791784
[2017-12-28 02:18:12] iter 20, loss: 1.19066405296, accuracy: 0.627551654176
[2017-12-28 02:18:12] iter 21, loss: 0.848123192787, accuracy: 0.688377014738
[2017-12-28 02:18:14] iter 22, loss: 1.64903652668, accuracy: 0.507685488594
[2017-12-28 02:18:14] iter 23, loss: 0.791278600693, accuracy: 0.845150434554
[2017-12-28 02:18:16] iter 24, loss: 0.546696007252, accuracy: 0.826051150925
[2017-12-28 02:18:19] iter 25, loss: 0.942895293236, accuracy: 0.683020391682
[2017-12-28 02:18:19] iter 26, loss: 1.53883314133, accuracy: 0.570599827565
[2017-12-28 02:18:21] iter 27, loss: 2.28536701202, accuracy: 0.416226713659
[2017-12-28 02:18:23] iter 28, loss: 0.407835692167, accuracy: 0.86289101762
[2017-12-28 02:18:24] iter 29, loss: 2.54308342934, accuracy: 0.463983192902
[2017-12-28 02:18:24] iter 30, loss: 1.96138679981, accuracy: 0.310141040922
[2017-12-28 02:18:24] iter 31, loss: 0.991347253323, accuracy: 0.613970731707
[2017-12-28 02:18:25] iter 32, loss: 0.33932954073, accuracy: 0.878425139822
[2017-12-28 02:18:26] iter 33, loss: 1.60865688324, accuracy: 0.586018566189
[2017-12-28 02:18:28] iter 34, loss: 0.242660358548, accuracy: 0.957648697093
[2017-12-28 02:18:31] iter 35, loss: 0.598027408123, accuracy: 0.852340612089
[2017-12-28 02:18:31] iter 36, loss: 0.722864329815, accuracy: 0.76006555179
[2017-12-28 02:18:33] iter 37, loss: 0.381607025862, accuracy: 0.931555007145
[2017-12-28 02:18:35] iter 38, loss: 0.720636129379, accuracy: 0.751221485369
[2017-12-28 02:18:38] iter 39, loss: 0.56502276659, accuracy: 0.82419365501
[2017-12-28 02:18:38] iter 40, loss: 0.711133956909, accuracy: 0.751996637243
[2017-12-28 02:18:40] iter 41, loss: 0.614671647549, accuracy: 0.821082996214
[2017-12-28 02:18:40] iter 42, loss: 1.23440897465, accuracy: 0.547263883808
[2017-12-28 02:18:41] iter 43, loss: 0.452299445868, accuracy: 0.854681271774
[2017-12-28 02:18:41] iter 44, loss: 0.425330966711, accuracy: 0.890518465355
[2017-12-28 02:18:43] iter 45, loss: 0.845939278603, accuracy: 0.785180578491
[2017-12-28 02:18:44] iter 46, loss: 0.955178797245, accuracy: 0.763815057485
[2017-12-28 02:18:44] iter 47, loss: 1.31424772739, accuracy: 0.539837110482
[2017-12-28 02:18:45] iter 48, loss: 1.70901429653, accuracy: 0.497242209241
[2017-12-28 02:18:47] iter 49, loss: 0.867735385895, accuracy: 0.73091854162
[2017-12-28 02:18:49] iter 50, loss: 1.43441331387, accuracy: 0.676181949977
[2017-12-28 02:18:51] iter 51, loss: 2.19334244728, accuracy: 0.310520130392
[2017-12-28 02:18:53] iter 52, loss: 0.637610554695, accuracy: 0.75858018752
[2017-12-28 02:18:55] iter 53, loss: 1.47474038601, accuracy: 0.611595936722
[2017-12-28 02:18:55] iter 54, loss: 0.374954462051, accuracy: 0.92994687915
[2017-12-28 02:18:57] iter 55, loss: 0.437835603952, accuracy: 0.889381581664
[2017-12-28 02:18:59] iter 56, loss: 0.43731456995, accuracy: 0.912188844228
[2017-12-28 02:19:02] iter 57, loss: 0.854432225227, accuracy: 0.732177947364
[2017-12-28 02:19:02] iter 58, loss: 0.20945635438, accuracy: 0.989018774354
[2017-12-28 02:19:04] iter 59, loss: 0.924484491348, accuracy: 0.753545569741
[2017-12-28 02:19:06] iter 60, loss: 1.10329842567, accuracy: 0.654651868844
[2017-12-28 02:19:08] iter 61, loss: 1.05957174301, accuracy: 0.649416470745
[2017-12-28 02:19:09] iter 62, loss: 1.2502374649, accuracy: 0.596253654971
[2017-12-28 02:19:11] iter 63, loss: 0.487723141909, accuracy: 0.848776216044
[2017-12-28 02:19:11] iter 64, loss: 0.634137451649, accuracy: 0.896780753754
[2017-12-28 02:19:13] iter 65, loss: 1.97178912163, accuracy: 0.335304384538
[2017-12-28 02:19:15] iter 66, loss: 0.609001517296, accuracy: 0.811427543901
[2017-12-28 02:19:17] iter 67, loss: 1.02552080154, accuracy: 0.670440858321
[2017-12-28 02:19:19] iter 68, loss: 0.391986787319, accuracy: 0.870227067982
[2017-12-28 02:19:21] iter 69, loss: 0.248874381185, accuracy: 0.938076472961
[2017-12-28 02:19:23] iter 70, loss: 0.933956623077, accuracy: 0.757388646288
[2017-12-28 02:19:23] iter 71, loss: 0.796231925488, accuracy: 0.753859814439
[2017-12-28 02:19:26] iter 72, loss: 0.79124134779, accuracy: 0.8053959674
[2017-12-28 02:19:26] iter 73, loss: 2.80600857735, accuracy: 0.274010571097
[2017-12-28 02:19:27] iter 74, loss: 0.464816331863, accuracy: 0.891929012346
[2017-12-28 02:19:29] iter 75, loss: 2.23450922966, accuracy: 0.27496923827
[2017-12-28 02:19:31] iter 76, loss: 1.35376966, accuracy: 0.598984234473
[2017-12-28 02:19:31] iter 77, loss: 1.0003105402, accuracy: 0.672216589288
[2017-12-28 02:19:32] iter 78, loss: 0.756433486938, accuracy: 0.68348998774
[2017-12-28 02:19:32] iter 79, loss: 1.02373111248, accuracy: 0.76376524526
[2017-12-28 02:19:33] iter 80, loss: 0.75917840004, accuracy: 0.816527729623
[2017-12-28 02:19:35] iter 81, loss: 0.853081047535, accuracy: 0.771369489999
[2017-12-28 02:19:37] iter 82, loss: 1.5049290657, accuracy: 0.534305753202
[2017-12-28 02:19:37] iter 83, loss: 1.68203949928, accuracy: 0.573133856053
[2017-12-28 02:19:38] iter 84, loss: 0.515508830547, accuracy: 0.782951230366
[2017-12-28 02:19:39] iter 85, loss: 1.20811128616, accuracy: 0.53466736126
[2017-12-28 02:19:41] iter 86, loss: 0.42307138443, accuracy: 0.894233459608
[2017-12-28 02:19:43] iter 87, loss: 0.391279250383, accuracy: 0.873099205569
[2017-12-28 02:19:44] iter 88, loss: 0.755724489689, accuracy: 0.83515411627
[2017-12-28 02:19:46] iter 89, loss: 1.44923615456, accuracy: 0.546603524475
[2017-12-28 02:19:46] iter 90, loss: 1.99888634682, accuracy: 0.505235356922
[2017-12-28 02:19:48] iter 91, loss: 0.620681762695, accuracy: 0.88680574198
[2017-12-28 02:19:49] iter 92, loss: 0.422692805529, accuracy: 0.957713499687
[2017-12-28 02:19:50] iter 93, loss: 0.544879198074, accuracy: 0.777944740248
[2017-12-28 02:19:51] iter 94, loss: 1.00489163399, accuracy: 0.79851559533
[2017-12-28 02:19:53] iter 95, loss: 2.3382499218, accuracy: 0.452357253369
[2017-12-28 02:19:55] iter 96, loss: 1.66060483456, accuracy: 0.63629121521
[2017-12-28 02:19:56] iter 97, loss: 0.649083912373, accuracy: 0.883461784344
[2017-12-28 02:19:58] iter 98, loss: 1.43939518929, accuracy: 0.710759098328
[2017-12-28 02:20:00] iter 99, loss: 3.50214552879, accuracy: 0.202204358686
[2017-12-28 02:20:00] iter 100, loss: 1.97785604, accuracy: 0.475317875841
[2017-12-28 02:20:01] iter 101, loss: 2.85605883598, accuracy: 0.171689542929
[2017-12-28 02:20:03] iter 102, loss: 1.18187367916, accuracy: 0.711930073399
[2017-12-28 02:20:03] iter 103, loss: 1.15132820606, accuracy: 0.646832325248
[2017-12-28 02:20:05] iter 104, loss: 0.581922709942, accuracy: 0.857827905808
[2017-12-28 02:20:06] iter 105, loss: 0.656112194061, accuracy: 0.79632157755
[2017-12-28 02:20:09] iter 106, loss: 2.58374929428, accuracy: 0.370096015843
[2017-12-28 02:20:09] iter 107, loss: 0.435693353415, accuracy: 0.963229173012
[2017-12-28 02:20:11] iter 108, loss: 0.717873632908, accuracy: 0.806690927136
[2017-12-28 02:20:13] iter 109, loss: 0.958994626999, accuracy: 0.754301248137
[2017-12-28 02:20:14] iter 110, loss: 0.116371661425, accuracy: 0.979702477344
[2017-12-28 02:20:16] iter 111, loss: 0.237539947033, accuracy: 0.965834495716
[2017-12-28 02:20:18] iter 112, loss: 1.48832702637, accuracy: 0.570966989385
[2017-12-28 02:20:19] iter 113, loss: 0.811153769493, accuracy: 0.771982898332
[2017-12-28 02:20:20] iter 114, loss: 1.24656116962, accuracy: 0.685535152787
[2017-12-28 02:20:22] iter 115, loss: 0.892456114292, accuracy: 0.699361517949
[2017-12-28 02:20:23] iter 116, loss: 0.153192147613, accuracy: 0.968979288567
[2017-12-28 02:20:25] iter 117, loss: 1.17936062813, accuracy: 0.61110262857
[2017-12-28 02:20:26] iter 118, loss: 0.86470502615, accuracy: 0.753268257357
[2017-12-28 02:20:28] iter 119, loss: 0.734274804592, accuracy: 0.739044280162
[2017-12-28 02:20:30] iter 120, loss: 1.4030829668, accuracy: 0.616333627005
[2017-12-28 02:20:31] iter 121, loss: 1.09395670891, accuracy: 0.658168684937
[2017-12-28 02:20:32] iter 122, loss: 1.17716634274, accuracy: 0.603737858272
[2017-12-28 02:20:32] iter 123, loss: 0.324292272329, accuracy: 0.954073375756
[2017-12-28 02:20:34] iter 124, loss: 0.532314896584, accuracy: 0.848021032281
[2017-12-28 02:20:36] iter 125, loss: 1.3212146759, accuracy: 0.626085448887
[2017-12-28 02:20:37] iter 126, loss: 0.394541621208, accuracy: 0.900351128544
[2017-12-28 02:20:38] iter 127, loss: 0.438501477242, accuracy: 0.900654402614
[2017-12-28 02:20:38] iter 128, loss: 0.222437262535, accuracy: 0.947888210356
[2017-12-28 02:20:40] iter 129, loss: 0.742925286293, accuracy: 0.724132775996
[2017-12-28 02:20:42] iter 130, loss: 0.338646173477, accuracy: 0.922570501534
[2017-12-28 02:20:43] iter 131, loss: 1.036911726, accuracy: 0.641563652136
[2017-12-28 02:20:43] iter 132, loss: 0.834488153458, accuracy: 0.727885628292
[2017-12-28 02:20:45] iter 133, loss: 0.30356541276, accuracy: 0.940932032137
[2017-12-28 02:20:47] iter 134, loss: 0.269528150558, accuracy: 0.934932362183
[2017-12-28 02:20:50] iter 135, loss: 0.797675549984, accuracy: 0.81142953795
[2017-12-28 02:20:51] iter 136, loss: 0.374357879162, accuracy: 0.877466396328
[2017-12-28 02:20:53] iter 137, loss: 1.6343215704, accuracy: 0.501809667168
[2017-12-28 02:20:55] iter 138, loss: 1.14530909061, accuracy: 0.569165194177
[2017-12-28 02:20:55] iter 139, loss: 0.272943586111, accuracy: 0.917086175802
[2017-12-28 02:20:57] iter 140, loss: 1.00281691551, accuracy: 0.613112500144
[2017-12-28 02:20:58] iter 141, loss: 1.23125350475, accuracy: 0.585266030014
[2017-12-28 02:21:00] iter 142, loss: 0.93706381321, accuracy: 0.627923136958
[2017-12-28 02:21:00] iter 143, loss: 0.995592832565, accuracy: 0.615635488569
[2017-12-28 02:21:02] iter 144, loss: 0.672787606716, accuracy: 0.815183411584
[2017-12-28 02:21:04] iter 145, loss: 0.197081997991, accuracy: 0.952952486381
[2017-12-28 02:21:05] iter 146, loss: 1.02722144127, accuracy: 0.707752823635
[2017-12-28 02:21:06] iter 147, loss: 0.643433511257, accuracy: 0.77436196888
[2017-12-28 02:21:06] iter 148, loss: 1.77850782871, accuracy: 0.304524373112
[2017-12-28 02:21:07] iter 149, loss: 0.595893919468, accuracy: 0.822775778633
[2017-12-28 02:21:08] iter 150, loss: 1.15214037895, accuracy: 0.654354415337
[2017-12-28 02:21:09] iter 151, loss: 0.933201611042, accuracy: 0.649251007365
[2017-12-28 02:21:10] iter 152, loss: 1.03272366524, accuracy: 0.73088266542
[2017-12-28 02:21:12] iter 153, loss: 0.315214067698, accuracy: 0.927817893396
[2017-12-28 02:21:12] iter 154, loss: 0.297203660011, accuracy: 0.930096755657
[2017-12-28 02:21:14] iter 155, loss: 0.852981090546, accuracy: 0.748378848403
[2017-12-28 02:21:16] iter 156, loss: 1.05011379719, accuracy: 0.740427604683
[2017-12-28 02:21:17] iter 157, loss: 0.465470671654, accuracy: 0.841422000282
[2017-12-28 02:21:19] iter 158, loss: 0.365146696568, accuracy: 0.879168943198
[2017-12-28 02:21:21] iter 159, loss: 1.1405633688, accuracy: 0.686789895697
[2017-12-28 02:21:23] iter 160, loss: 0.743581652641, accuracy: 0.859566899451
[2017-12-28 02:21:23] iter 161, loss: 2.67128610611, accuracy: 0.235103738332
[2017-12-28 02:21:24] iter 162, loss: 1.26941847801, accuracy: 0.663607941222
[2017-12-28 02:21:25] iter 163, loss: 0.535805523396, accuracy: 0.843289894642
[2017-12-28 02:21:26] iter 164, loss: 1.04693651199, accuracy: 0.841250908145
[2017-12-28 02:21:26] iter 165, loss: 1.46448123455, accuracy: 0.626515514761
[2017-12-28 02:21:27] iter 166, loss: 0.767644584179, accuracy: 0.808138959454
[2017-12-28 02:21:29] iter 167, loss: 0.521022796631, accuracy: 0.882695305031
[2017-12-28 02:21:31] iter 168, loss: 0.266331136227, accuracy: 0.882824065288
[2017-12-28 02:21:34] iter 169, loss: 0.797376155853, accuracy: 0.779932101407
[2017-12-28 02:21:34] iter 170, loss: 0.64709508419, accuracy: 0.953511841784
[2017-12-28 02:21:34] iter 171, loss: 1.57596516609, accuracy: 0.548644387424
[2017-12-28 02:21:37] iter 172, loss: 2.09859728813, accuracy: 0.505560020536
[2017-12-28 02:21:37] iter 173, loss: 1.29463338852, accuracy: 0.608598548297
[2017-12-28 02:21:38] iter 174, loss: 1.72180783749, accuracy: 0.458769931663
[2017-12-28 02:21:38] iter 175, loss: 2.44861769676, accuracy: 0.381859216184
[2017-12-28 02:21:40] iter 176, loss: 1.24719178677, accuracy: 0.547194467509
[2017-12-28 02:21:42] iter 177, loss: 1.04720675945, accuracy: 0.549874461308
[2017-12-28 02:21:42] iter 178, loss: 1.51679217815, accuracy: 0.472512475812
[2017-12-28 02:21:44] iter 179, loss: 1.23863005638, accuracy: 0.693468414962
[2017-12-28 02:21:46] iter 180, loss: 0.355862557888, accuracy: 0.904180694601
[2017-12-28 02:21:46] iter 181, loss: 0.406710684299, accuracy: 0.961193466151
[2017-12-28 02:21:48] iter 182, loss: 2.64690542221, accuracy: 0.309677398002
[2017-12-28 02:21:50] iter 183, loss: 0.471818089485, accuracy: 0.869741223916
[2017-12-28 02:21:52] iter 184, loss: 0.487083226442, accuracy: 0.863880795084
[2017-12-28 02:21:52] iter 185, loss: 0.361948162317, accuracy: 0.937945129126
[2017-12-28 02:21:54] iter 186, loss: 0.684942245483, accuracy: 0.699282306728
[2017-12-28 02:21:56] iter 187, loss: 0.400614082813, accuracy: 0.874508948058
[2017-12-28 02:21:58] iter 188, loss: 0.652782320976, accuracy: 0.840428794944
[2017-12-28 02:21:59] iter 189, loss: 2.01617527008, accuracy: 0.218300734251
[2017-12-28 02:22:01] iter 190, loss: 0.664151370525, accuracy: 0.813598839771
[2017-12-28 02:22:03] iter 191, loss: 0.231789410114, accuracy: 0.948567464568
[2017-12-28 02:22:05] iter 192, loss: 1.08970463276, accuracy: 0.724340304528
[2017-12-28 02:22:07] iter 193, loss: 1.09184813499, accuracy: 0.52857878276
[2017-12-28 02:22:07] iter 194, loss: 0.146166190505, accuracy: 0.954310465279
[2017-12-28 02:22:08] iter 195, loss: 0.57039552927, accuracy: 0.926556394895
[2017-12-28 02:22:09] iter 196, loss: 1.82270336151, accuracy: 0.483627086736
[2017-12-28 02:22:11] iter 197, loss: 4.93965578079, accuracy: 0.169291693703
[2017-12-28 02:22:12] iter 198, loss: 1.54355704784, accuracy: 0.531464224941
[2017-12-28 02:22:12] iter 199, loss: 0.21259765327, accuracy: 0.937185349406
class [0], IoU: 0.646121799946
class [1], IoU: 0.749336481094
class [2], IoU: 0.880544066429
class [3], IoU: 0.642751812935
class [4], IoU: 0.74165648222
class [5], IoU: 0.686927437782
class [6], IoU: 0.764645218849
class [7], IoU: 0.673678636551
class [8], IoU: 0.537444889545
class [9], IoU: 0.578303515911
class [10], IoU: 0.39261418581
class [11], IoU: 0.497854530811
class [12], IoU: 0.693387508392
class [13], IoU: 0.3501714468
class [14], IoU: 0.22074533999
class [15], IoU: 0.369570881128
class [16], IoU: 0.579058945179
class [17], IoU: 0.398170232773
class [18], IoU: 0.607509374619
class [19], IoU: 0.33067458868
class [20], IoU: 0.800129175186
class [21], IoU: 0.538591861725
class [22], IoU: 0.69921708107
class [23], IoU: 0.217400863767
class [24], IoU: 0.481257975101
class [25], IoU: 0.0424569509923
class [26], IoU: 0.74632704258
class [27], IoU: 0.34174951911
class [28], IoU: 0.0937786325812
class [29], IoU: 0.0
class [30], IoU: 0.0436393767595
class [31], IoU: 0.109169155359
class [32], IoU: 0.304582118988
class [33], IoU: 0.0680125132203
class [34], IoU: 0.283125668764
class [35], IoU: 0.169976443052
class [36], IoU: 0.31747302413
class [37], IoU: 0.417369812727
class [38], IoU: 0.0151301845908
class [39], IoU: 0.162476941943
class [40], IoU: 0.114724546671
class [41], IoU: 0.242827609181
class [42], IoU: 0.103289596736
class [43], IoU: 0.163229823112
class [44], IoU: 0.0
class [45], IoU: 0.400297671556
class [46], IoU: 0.0473488941789
class [47], IoU: 0.488892793655
class [48], IoU: 0.433681428432
class [49], IoU: 0.722869694233
class [50], IoU: 0.0937624648213
class [51], IoU: 0.133814200759
class [52], IoU: 0.0524854399264
class [53], IoU: 0.0400343649089
class [54], IoU: 0.0
class [55], IoU: 0.0
class [56], IoU: 0.740742743015
class [57], IoU: 0.390477776527
class [58], IoU: 0.225301772356
class [59], IoU: 0.505782425404
class [60], IoU: 0.0
class [61], IoU: 0.482456564903
class [62], IoU: 0.209731698036
class [63], IoU: 0.171252861619
class [64], IoU: 0.183441638947
class [65], IoU: 0.63432264328
class [66], IoU: 0.154032751918
class [67], IoU: 0.322409629822
class [68], IoU: 0.0836538076401
class [69], IoU: 0.0224782507867
class [70], IoU: 0.373656272888
class [71], IoU: 0.0716027989984
class [72], IoU: 0.432568788528
class [73], IoU: 0.495651215315
class [74], IoU: 0.262813061476
class [75], IoU: 0.268429607153
class [76], IoU: 0.187706112862
class [77], IoU: 0.116673178971
class [78], IoU: 0.0
class [79], IoU: 0.0
class [80], IoU: 0.0
class [81], IoU: 0.0302180945873
class [82], IoU: 0.089597016573
class [83], IoU: 0.469484508038
class [84], IoU: 0.253204405308
class [85], IoU: 0.359681338072
class [86], IoU: 0.0899045243859
class [87], IoU: 0.0201730821282
class [88], IoU: 0.0
class [89], IoU: 0.228334590793
class [90], IoU: 0.0
class [91], IoU: 0.0
class [92], IoU: 0.0
class [93], IoU: 0.0422725863755
class [94], IoU: 0.0
class [95], IoU: 0.0
class [96], IoU: 0.0333762541413
class [97], IoU: 0.265952885151
class [98], IoU: 0.367960900068
class [99], IoU: 0.0319736115634
class [100], IoU: 0.0273036286235
class [101], IoU: 0.0
class [102], IoU: 0.0
class [103], IoU: 0.0
class [104], IoU: 0.0
class [105], IoU: 0.0
class [106], IoU: 0.0
class [107], IoU: 0.790850400925
class [108], IoU: 0.0
class [109], IoU: 0.0
class [110], IoU: 0.0582770928741
class [111], IoU: 0.0
class [112], IoU: 0.0
class [113], IoU: 0.0
class [114], IoU: 0.0
class [115], IoU: 0.0
class [116], IoU: 0.581868767738
class [117], IoU: 0.438861310482
class [118], IoU: 0.0
class [119], IoU: 0.0
class [120], IoU: 0.0
class [121], IoU: 0.0
class [122], IoU: 0.0520976595581
class [123], IoU: 0.084482178092
class [124], IoU: 0.807438015938
class [125], IoU: 0.0353396423161
class [126], IoU: 0.578522205353
class [127], IoU: 0.434498429298
class [128], IoU: 0.0
class [129], IoU: 0.0
class [130], IoU: 0.00210757413879
class [131], IoU: 0.0
class [132], IoU: 0.164425864816
class [133], IoU: 0.0
class [134], IoU: 0.0343745276332
class [135], IoU: 0.131438016891
class [136], IoU: 0.0
class [137], IoU: 0.025352967903
class [138], IoU: 0.0
class [139], IoU: 0.216375529766
class [140], IoU: 0.023335961625
class [141], IoU: 0.0
class [142], IoU: 0.0
class [143], IoU: 0.0
class [144], IoU: 0.161207780242
class [145], IoU: 0.0
class [146], IoU: 0.0
class [147], IoU: 0.000212699960684
class [148], IoU: 0.0
class [149], IoU: 0.0
[Eval Summary]:
Loss: 0.982096950412, Mean IoU: 0.2233, Accurarcy: 72.89%
Evaluation Done!
